import argparse
import os
import torch
from exp.exp_main import Exp_Main
import random
import numpy as np
fix_seed = 2021
random.seed(fix_seed)
torch.manual_seed(fix_seed)
np.random.seed(fix_seed)

import argparse
import os
import torch
import random
import numpy as np
import pandas as pd
from datetime import timedelta
from datetime import datetime


parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')

# # basic config
from data_provider.data_loader import Dataset_Custom
import argparse
import os
import torch
import random
import numpy as np
import pandas as pd
from datetime import timedelta
from datetime import datetime

parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')

# basic config
parser.add_argument('--is_training', type=int, default=1, help='status')
parser.add_argument('--model_id', type=str, default='mirror', help='model id')
parser.add_argument('--model', type=str, default='Autoformer',
                    help='model name, options: [Autoformer, Informer, Transformer]')

# data loader/home/data_ming/order/Autoformer-main
parser.add_argument('--data', type=str, default='custom', help='dataset type')
parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')
parser.add_argument('--features', type=str, default='M',
                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')
parser.add_argument('--freq', type=str, default='h',
                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')
parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')

# forecasting task
parser.add_argument('--seq_len', type=int, default=15, help='input sequence length')
parser.add_argument('--label_len', type=int, default=1, help='start token length')
parser.add_argument('--pred_len', type=int, default=1, help='prediction sequence length')

# model define
parser.add_argument('--enc_in', type=int, default=124, help='encoder input size')
parser.add_argument('--dec_in', type=int, default=124, help='decoder input size')
parser.add_argument('--c_out', type=int, default=124, help='output size')
parser.add_argument('--d_model', type=int, default=512, help='dimension of model')
parser.add_argument('--n_heads', type=int, default=8, help='num of heads')
parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')
parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')
parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')
parser.add_argument('--moving_avg', type=int, default=7, help='window size of moving average')
parser.add_argument('--factor', type=int, default=1, help='attn factor')
parser.add_argument('--distil', action='store_false',
                    help='whether to use distilling in encoder, using this argument means not using distilling',
                    default=True)
parser.add_argument('--dropout', type=float, default=0.05, help='dropout')
parser.add_argument('--embed', type=str, default='timeF',
                    help='time features encoding, options:[timeF, fixed, learned]')
parser.add_argument('--activation', type=str, default='gelu', help='activation')
parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')
parser.add_argument('--do_predict', default=False, help='whether to predict unseen future data')

# optimizationS
parser.add_argument('--num_workers', type=int, default=0, help='data loader num workers')
parser.add_argument('--itr', type=int, default=2, help='experiments times')
parser.add_argument('--train_epochs', type=int, default=1, help='train epochs')
parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')
parser.add_argument('--patience', type=int, default=2, help='early stopping patience')
parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')
parser.add_argument('--des', type=str, default='test', help='exp description')
parser.add_argument('--loss', type=str, default='mse', help='loss function')
parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')
parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)

# GPU
parser.add_argument('--use_gpu', type=bool, default=False, help='use gpu')
parser.add_argument('--gpu', type=int, default=0, help='gpu')
parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)
parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')

args = parser.parse_args()

args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False

if args.use_gpu and args.use_multi_gpu:
    args.dvices = args.devices.replace(' ', '')
    device_ids = args.devices.split(',')
    args.device_ids = [int(id_) for id_ in device_ids]
    args.gpu = args.device_ids[0]

print('Args in experiment:')
print(args)



import findspark
findspark.init()
#添加此代码
from pyspark import SparkConf, SparkContext

import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import os 
import gc 
# import datetime
from pyspark.sql import SparkSession

import argparse
import os
import torch
from exp.exp_main import Exp_Main
import random
import numpy as np
from datetime import timedelta
from datetime import datetime



spark = SparkSession.builder \
    .appName("mirror_corder_day_all") \
    .config("hive.metastore.uris","thrift://hive-meta-marketth.hive.svc.datacloud.17usoft.com:9083") \
    .config("hive.metastore.local", "false") \
    .config("spark.io.compression.codec", "snappy") \
    .config("spark.sql.execution.arrow.enabled", "false") \
    .enableHiveSupport() \
    .getOrCreate()

data_sql = '''

SELECT intotime as date
    ,uv as OT
    ,platname,productid,futureuv,futureintotime,intotimechunyun,intotimeholiday,intotimequarter,intotimeworkday
    ,futuredatechunyun,futuredateholiday,futuredatequarter,futuredateworkday,hours,futurehours,elongapp,tcapp,wechat,intotimeweek,futuredateweek,intotimeday,futuredateday,intotimemonth,futuredatemonth,intotimeweekofyear
    ,futuredateweekofyear,intotimedayofyear,futuredatedayofyear,intotimeyear,futuredateyear,futurehoursteps,lag1uv,lag2uv,lag3uv,lag4uv,lag5uv
    ,lag6uv,lag7uv,lag8uv,lag9uv,lag10uv,lag11uv,lag12uv,lag13uv,lag14uv,lag15uv,lag16uv,lag17uv,lag18uv,lag19uv,lag20uv,lag21uv,lag22uv,lag23uv,lag24uv,lag25uv,lag26uv,lag27uv,lag28uv,lag29uv
    ,lag30uv,lag31uv,lag32uv,lag33uv,lag34uv,lag35uv,lag36uv,lag37uv,lag38uv,lag39uv,lag40uv,lag41uv,lag42uv,lag43uv,lag44uv,lag45uv,lag46uv,lag47uv,lag48uv,lag49uv,lag50uv,lag51uv
    ,lag52uv,lag53uv,lag54uv,lag55uv,lag56uv,lag57uv,lag58uv,lag59uv,lag60uv,lag30maxuv,lag30minuv,lag30sumuv,lag30avguv,lag12maxuv,lag12minuv,lag12sumuv,lag12avguv,lag3maxuv,lag3minuv
    ,lag3sumuv,lag3avguv,lag7maxuv,lag7minuv,lag7sumuv,lag7avguv,weekavguv,weekmaxuv,weekminuv,houravguv,hourmaxuv,hourminuv,dayofmonthavguv,dayofmonthmaxuv,dayofmonthminuv,weekofyearavguv,weekofyearmaxuv
    ,weekofyearminuv,futuredateweekhour,intodayofholiday,intodaysofnextholiday,intoholidaynameid,futuredatedayofholiday,futuredatedaysofnextholiday,futuredateholidaynameid,holidayavguv,holidaymaxuv,holidayminuv
    ,hourpieces0006,hourpieces0612,hourpieces1218,hourpieces1824,lag6maxuv,lag6minuv,lag6sumuv,lag6avguv,futuredatehourpiece,hourpieceavguv,hourpiecemaxuv,hourpieceminuv
FROM (
    SELECT intotime,platname,productid,uv,futureintotime,futureuv,intotimechunyun,intotimeholiday,intotimequarter,intotimeworkday
    ,futuredatechunyun,futuredateholiday,futuredatequarter,futuredateworkday,hours,futurehours,elongapp,tcapp,wechat,intotimeweek,futuredateweek,intotimeday,futuredateday,intotimemonth,futuredatemonth,intotimeweekofyear
    ,futuredateweekofyear,intotimedayofyear,futuredatedayofyear,intotimeyear,futuredateyear,futurehoursteps,lag1uv,lag2uv,lag3uv,lag4uv,lag5uv
    ,lag6uv,lag7uv,lag8uv,lag9uv,lag10uv,lag11uv,lag12uv,lag13uv,lag14uv,lag15uv,lag16uv,lag17uv,lag18uv,lag19uv,lag20uv,lag21uv,lag22uv,lag23uv,lag24uv,lag25uv,lag26uv,lag27uv,lag28uv,lag29uv
    ,lag30uv,lag31uv,lag32uv,lag33uv,lag34uv,lag35uv,lag36uv,lag37uv,lag38uv,lag39uv,lag40uv,lag41uv,lag42uv,lag43uv,lag44uv,lag45uv,lag46uv,lag47uv,lag48uv,lag49uv,lag50uv,lag51uv
    ,lag52uv,lag53uv,lag54uv,lag55uv,lag56uv,lag57uv,lag58uv,lag59uv,lag60uv,lag30maxuv,lag30minuv,lag30sumuv,lag30avguv,lag12maxuv,lag12minuv,lag12sumuv,lag12avguv,lag3maxuv,lag3minuv
    ,lag3sumuv,lag3avguv,lag7maxuv,lag7minuv,lag7sumuv,lag7avguv,weekavguv,weekmaxuv,weekminuv,houravguv,hourmaxuv,hourminuv,dayofmonthavguv,dayofmonthmaxuv,dayofmonthminuv,weekofyearavguv,weekofyearmaxuv
    ,weekofyearminuv,futuredateweekhour,intodayofholiday,intodaysofnextholiday,intoholidaynameid,futuredatedayofholiday,futuredatedaysofnextholiday,futuredateholidaynameid,holidayavguv,holidaymaxuv,holidayminuv
    ,hourpieces0006,hourpieces0612,hourpieces1218,hourpieces1824,lag6maxuv,lag6minuv,lag6sumuv,lag6avguv,futuredatehourpiece,hourpieceavguv,hourpiecemaxuv,hourpieceminuv
    ,ROW_NUMBER()  OVER( PARTITION BY  platname,productid,uv ORDER BY intotime DESC) AS rankno 
    FROM(
        SELECT *
        FROM app_dm.rpt_dc_mirror_uv_hour_model_dateset     
        WHERE  d>='20220620' AND TO_DATE(intotime) > '2020-01-01' AND platname = '汇总' AND productid = '999'
        AND (unix_timestamp(futureintotime) - unix_timestamp(intotime)) = 3600
        )a
)b
WHERE b.rankno = 1
ORDER BY intotime 

'''
data = spark .sql(data_sql).toPandas()
print(type(data))
# df_raw = pd.read_csv('./dataset/exchange_rate/dataset_0413_all.csv')
print(data.shape)
print(data.head(5))
data = data.drop(['platname','productid','futureintotime','futureuv'],axis=1) 


auto_result = []

df_raw = data
cols = list(df_raw.columns)
cols.remove(args.target)
cols.remove('date')
df_raw = df_raw[['date'] + cols + [args.target]]
df_stamp = df_raw[['date']]
if args.features == 'M' or args.features == 'MS':
    cols_data = df_raw.columns[1:]
    df_data = df_raw[cols_data]
elif args.features == 'S':
    df_data = df_raw[[self.target]]
print(df_data.shape)
Exp = Exp_Main
parser.add_argument("--raw_data", type=type(df_data),default=df_data)
parser.add_argument("--raw_stamp", type=type(df_stamp),default=df_stamp)
args = parser.parse_args()
args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False

if args.use_gpu and args.use_multi_gpu:
    args.dvices = args.devices.replace(' ', '')
    device_ids = args.devices.split(',')
    args.device_ids = [int(id_) for id_ in device_ids]
    args.gpu = args.device_ids[0]
args = parser.parse_args()
args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False
#     print('Args in experiment:')
#     print(args)
if args.is_training:
    for ii in range(args.itr):
        # setting record of experiments
        setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(
            args.model_id,
            args.model,
            args.data,
            args.features,
            args.seq_len,
            args.label_len,
            args.pred_len,
            args.d_model,
            args.n_heads,
            args.e_layers,
            args.d_layers,
            args.d_ff,
            args.factor,
            args.embed,
            args.distil,
            args.des, ii)

        exp = Exp(args)  # set experiments
        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))
        print(args.use_gpu)
        print(args.use_amp)
        exp.train(setting)

        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))
        result_aut = exp.test(setting)
        print(result_aut)  
        result_aut_dataframe=pd.DataFrame(result_aut,columns=('true_result','pred_result','err','predict_time'))
        print(result_aut_dataframe.shape)
        spark.createDataFrame(result_aut_dataframe).write.mode("overwrite").format("hive").saveAsTable('tmp_dm.tmp_xwj_mirror_uv_hour_all')
        
#         print('test')
# #             auto_result.append(exp.test(setting))
# #             print(auto_result)
#             print('预测hour')
#             print(i)
#             future_value


    if args.do_predict:
        print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))
        exp.predict(setting, True)

        torch.cuda.empty_cache()
else:
    ii = 0
    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(args.model_id,
                                                                                                  args.model,                                                                                                               args.data,
                                                                                                  args.features,
                                                                                                  args.seq_len,
                                                                                                  args.label_len,
                                                                                                  args.pred_len,
                                                                                                  args.d_model,
                                                                                                  args.n_heads,
                                                                                                  args.e_layers,
                                                                                                  args.d_layers,
                                                                                                  args.d_ff,
                                                                                                  args.factor,
                                                                                                  args.embed,
                                                                                                  args.distil,
                                                                                                  args.des, ii)

    exp = Exp(args)  # set experiments
    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))
    exp.test(setting, test=1)
    torch.cuda.empty_cache()



    

    
    
    
#     print(parser._actions)
#     print(parser._actions[42])
#     print(parser._actions[43])
#     auto_result.append([days,'大盘',i,preds,trues])
    parser._handle_conflict_resolve(None, [('--raw_stamp',parser._actions[43])])
    parser._handle_conflict_resolve(None, [('--raw_data',parser._actions[42])])
# print(auto_result)
    






# dataset=df_raw.loc[df_raw['platname'] == '汇总']
# dataset=dataset.loc[dataset['dtype'] == 0]
# dataset=dataset.loc[dataset['productid'] == 999]
# dataset=dataset.loc[dataset['futuredaysteps'] == 1]
# dataset = dataset.drop(['futurecdate','productid','platname','dtype'],axis=1)
# dataset = dataset.rename(columns={'futuredorder': 'OT'})
# df_raw = dataset.rename(columns={'cdate': 'date'})

# cols = list(df_raw.columns)
# cols.remove(args.target)
# cols.remove('date')
# df_raw = df_raw[['date'] + cols + [args.target]]
# df_stamp = df_raw[['date']]
# if args.features == 'M' or args.features == 'MS':
#     cols_data = df_raw.columns[1:]
#     df_data = df_raw[cols_data]
# elif args.features == 'S':
#     df_data = df_raw[[self.target]]
# print(df_data.shape)
# Exp = Exp_Main

# parser.add_argument("--raw_data", type=type(df_data),default=df_data)
# parser.add_argument("--raw_stamp", type=type(df_stamp),default=df_stamp)
# args = parser.parse_args()
# args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False

# if args.use_gpu and args.use_multi_gpu:
#     args.dvices = args.devices.replace(' ', '')
#     device_ids = args.devices.split(',')
#     args.device_ids = [int(id_) for id_ in device_ids]
#     args.gpu = args.device_ids[0]
# args = parser.parse_args()
# args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False
# print('Args in experiment:')
# print(args)
# # print(parser._actions)

# if args.is_training:
#     for ii in range(args.itr):
#         # setting record of experiments
#         setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(
#         args.model_id,
#         args.model,
#         args.data,
#         args.features,
#         args.seq_len,
#         args.label_len,
#         args.pred_len,
#         args.d_model,
#         args.n_heads,
#         args.e_layers,
#         args.d_layers,
#         args.d_ff,
#         args.factor,
#         args.embed,
#         args.distil,
#         args.des, ii)

#         exp = Exp(args)  # set experiments
#         print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))
#         print(args.use_gpu)
#         print(args.use_amp)
#         exp.train(setting)

#         print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))
#         exp.test(setting)

#         if args.do_predict:
#             print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))
#             exp.predict(setting, True)

#         torch.cuda.empty_cache()
# else:
#     ii = 0
#     setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(args.model_id,
#                                                                                                   args.model,                                                                                                               args.data,
#                                                                                                   args.features,
#                                                                                                   args.seq_len,
#                                                                                                   args.label_len,
#                                                                                                   args.pred_len,
#                                                                                                   args.d_model,
#                                                                                                   args.n_heads,
#                                                                                                   args.e_layers,
#                                                                                                   args.d_layers,
#                                                                                                   args.d_ff,
#                                                                                                   args.factor,
#                                                                                                   args.embed,
#                                                                                                   args.distil,
#                                                                                                   args.des, ii)

#     exp = Exp(args)  # set experiments
#     print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))
#     exp.test(setting, test=1)
#     torch.cuda.empty_cache()



    
    
    
    
    
    
# #     lastday = df_stamp.loc[df_stamp.index[-1],'date']
# #     y = datetime.strptime(lastday, '%Y-%m-%d').date()
# #     add_day = y + timedelta(days=i)
# #     new = pd.DataFrame({'date':add_day},index=[0])
# #     df_stamp=df_stamp.append(new,ignore_index=True)
# #     print(df_stamp.shape)
    
    
# #     pred = exp.test
# #     df_data.loc[df_data.index.max() + 1] = pred[-1]
    
    
#     parser._handle_conflict_resolve(None, [('--raw_stamp',parser._actions[45])])
#     parser._handle_conflict_resolve(None, [('--raw_data',parser._actions[44])])
# #     print(len(parser._actions))













